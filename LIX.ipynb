{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"news_sample.csv\", dtype={0: str, 1: str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Readability' from 'readability' (/Users/andreazeuthenheidam/miniconda3/lib/python3.11/site-packages/readability/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[51], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreadability\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Readability\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mreadability\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mexceptions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ReadabilityException\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Function to compute LIX number for a given text using py-readability-metrics\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Readability' from 'readability' (/Users/andreazeuthenheidam/miniconda3/lib/python3.11/site-packages/readability/__init__.py)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from readability import Readability\n",
    "from readability.exceptions import ReadabilityException\n",
    "\n",
    "# Function to compute LIX number for a given text using py-readability-metrics\n",
    "def compute_lix_py_readability(text):\n",
    "    try:\n",
    "        readability = Readability(text)\n",
    "        return readability.lix()\n",
    "    except ReadabilityException:\n",
    "        return None\n",
    "\n",
    "# Apply LIX computation function to each article\n",
    "df['lix'] = df['content'].apply(compute_lix_py_readability)\n",
    "\n",
    "# Drop rows with missing LIX values\n",
    "df.dropna(subset=['lix'], inplace=True)\n",
    "\n",
    "# Sort the DataFrame based on LIX numbers\n",
    "df_sorted = df.sort_values(by='lix', ascending=False)\n",
    "\n",
    "# Print top 10 articles with highest LIX numbers\n",
    "print(\"Top 10 articles with highest LIX numbers:\")\n",
    "print(df_sorted.head(10)[['content', 'lix']])\n",
    "\n",
    "# Print top 10 articles with lowest LIX numbers\n",
    "print(\"\\nTop 10 articles with lowest LIX numbers:\")\n",
    "print(df_sorted.tail(10)[['content', 'lix']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 articles with highest LIX numbers:\n",
      "                                               content         lix\n",
      "167  I Was Silent When They Came for You … So There...  115.278261\n",
      "27   Before It's News ©\\n\\npeople powered news ®\\n\\...   65.338462\n",
      "185  Fermented vegetables\\n\\nKefir\\n\\nKombucha\\n\\nA...   58.744769\n",
      "224  GOLD PRICES slipped Tuesday in London’s wholes...   52.122722\n",
      "228  By Pam Martens and Russ Martens: April 4, 2016...   48.238029\n",
      "13   Mission must be at the heart of journalism, Po...   45.516470\n",
      "64   Subscribe to Canada Free Press for FREE\\n\\nMaj...   43.741002\n",
      "49   5 Scandalous Reasons Big Finance Is Trying Har...   41.551252\n",
      "133  By\\n\\n21st Century Wire says…\\n\\nUK Column anc...   41.420732\n",
      "71   Subscribe to Canada Free Press for FREE\\n\\nCha...   40.969738\n",
      "\n",
      "Top 10 articles with lowest LIX numbers:\n",
      "                                               content        lix\n",
      "203  Trump arrives in Davos…\\n\\n% of readers think ...  12.339669\n",
      "62   54\\n\\nBy Looking at Lyme Disease on Friday Dec...  12.250000\n",
      "121  Posted on by willyloman\\n\\nHis opening stateme...  11.915323\n",
      "177  Cancer causes\\n\\n% of readers think this story...  11.590643\n",
      "100  GreenMedInfo – Action Item link\\n\\n% of reader...  10.994792\n",
      "16   SIGNS AND WONDERS\\n\\nSNAP Out of It. The Bible...  10.559140\n",
      "41   Excerpts from Carl Sagan’s Cosmos. More specif...  10.300000\n",
      "221  2016 Food Safety Speech in New Zealand\\n\\n% of...   9.902083\n",
      "227  Is There Something Else Going-On Many of Us ha...   9.763158\n",
      "197  Is There Something Else Going-On Many of Us ha...   9.763158\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Function to compute LIX number for a given text using NLTK\n",
    "def compute_lix_nltk(text):\n",
    "    words = word_tokenize(text)\n",
    "    num_words = len(words)\n",
    "    sentences = sent_tokenize(text)\n",
    "    num_sentences = len(sentences)\n",
    "    long_words = [word for word in words if len(word) >= 6]\n",
    "    num_long_words = len(long_words)\n",
    "    avg_sentence_length = num_words / num_sentences\n",
    "    percentage_long_words = (num_long_words / num_words) * 100\n",
    "    lix = avg_sentence_length + (percentage_long_words / 100)\n",
    "    return lix\n",
    "\n",
    "# Apply LIX computation function to each article\n",
    "df['lix'] = df['content'].apply(compute_lix_nltk)\n",
    "\n",
    "# Sort the DataFrame based on LIX numbers\n",
    "df_sorted = df.sort_values(by='lix', ascending=False)\n",
    "\n",
    "# Print top 10 articles with highest LIX numbers\n",
    "print(\"Top 10 articles with highest LIX numbers:\")\n",
    "print(df_sorted.head(10)[['content', 'lix']])\n",
    "\n",
    "# Print top 10 articles with lowest LIX numbers\n",
    "print(\"\\nTop 10 articles with lowest LIX numbers:\")\n",
    "print(df_sorted.tail(10)[['content', 'lix']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average LIX Number for Fake News: 46.87855106919095\n",
      "Average LIX Number for Reliable News: 48.418802929104245\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Define the mapping of labels to categories\n",
    "label_to_category = {\n",
    "    \"fake\": \"fake\",\n",
    "    \"satire\": \"fake\",\n",
    "    \"bias\": \"fake\",\n",
    "    \"conspiracy\": \"fake\",\n",
    "    \"junksci\": \"fake\",\n",
    "    \"reliable\": \"reliable\",\n",
    "    \"political\": \"reliable\",\n",
    "    \"clickbait\": \"reliable\",\n",
    "}\n",
    "\n",
    "# Apply the mapping to create a 'category' column in the DataFrame\n",
    "df['category'] = df['type'].apply(lambda x: label_to_category.get(x, 'omitted'))\n",
    "\n",
    "# Function to compute the LIX number for a given text\n",
    "def compute_lix(text):\n",
    "    # Tokenize the text into sentences\n",
    "    sentences = sent_tokenize(text)\n",
    "    \n",
    "    # Tokenize each sentence into words\n",
    "    words = [word_tokenize(sentence) for sentence in sentences]\n",
    "    \n",
    "    # Flatten the list of words\n",
    "    words = [word for sublist in words for word in sublist]\n",
    "    \n",
    "    # Count the total number of words\n",
    "    W = len(words)\n",
    "    \n",
    "    # Count the number of long words (words with more than 6 letters)\n",
    "    L = sum(1 for word in words if len(word) > 6)\n",
    "    \n",
    "    # Count the number of sentences\n",
    "    S = len(sentences)\n",
    "    \n",
    "    # Compute the LIX number\n",
    "    lix = (W/S) + (L*100/W)\n",
    "    \n",
    "    return lix\n",
    "\n",
    "# Iterate over each article in the dataset\n",
    "category_lix = {\"fake\": [], \"reliable\": []}\n",
    "for index, row in df.iterrows():\n",
    "    # Check if the article is assigned to a category\n",
    "    if row['category'] != 'omitted':\n",
    "        # Compute the LIX number for the content of the article\n",
    "        lix = compute_lix(row['content'])\n",
    "        \n",
    "        # Assign the article to its respective category\n",
    "        category = row['category']\n",
    "        \n",
    "        # Store the LIX number for the corresponding category\n",
    "        category_lix[category].append(lix)\n",
    "\n",
    "# Compute the average LIX number for each category\n",
    "average_lix = {category: sum(lix_list) / len(lix_list) for category, lix_list in category_lix.items()}\n",
    "\n",
    "print(\"Average LIX Number for Fake News:\", average_lix[\"fake\"])\n",
    "print(\"Average LIX Number for Reliable News:\", average_lix[\"reliable\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average LIX Number for Fake News: 24.965707944219986\n",
      "Average LIX Number for Reliable News: 25.352005772634353\n"
     ]
    }
   ],
   "source": [
    "# Filter the DataFrame for fake news and calculate the average LIX number\n",
    "fake_news_average_lix = df[df['category'] == 'fake']['lix'].mean()\n",
    "\n",
    "# Filter the DataFrame for reliable news and calculate the average LIX number\n",
    "reliable_news_average_lix = df[df['category'] == 'reliable']['lix'].mean()\n",
    "\n",
    "print(\"Average LIX Number for Fake News:\", fake_news_average_lix)\n",
    "print(\"Average LIX Number for Reliable News:\", reliable_news_average_lix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain_assigned_types_all = df.groupby('domain')['type'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Domain with the highest average LIX:\n",
      "Domain: washingtonsblog.com\n",
      "Average LIX: 115.27826086956522\n",
      "\n",
      "Domain with the lowest average LIX:\n",
      "Domain: collectivelyconscious.net\n",
      "Average LIX: 10.3\n",
      "\n",
      "Average LIX by domain:\n",
      "domain\n",
      "21stcenturywire.com           41.420732\n",
      "alternet.org                  30.208814\n",
      "americanlookout.com           31.677070\n",
      "anonhq.com                    31.710311\n",
      "awarenessact.com              19.592501\n",
      "awm.com                       18.182367\n",
      "barenakedislam.com            33.723881\n",
      "beforeitsnews.com             23.734963\n",
      "bipartisanreport.com          20.836056\n",
      "blackagendareport.com         27.296296\n",
      "breakpoint.org                10.559140\n",
      "breitbart.com                 24.616655\n",
      "canadafreepress.com           28.990978\n",
      "charismanews.com              22.712474\n",
      "christianpost.com             23.471175\n",
      "city-journal.org              21.285714\n",
      "cnnnext.com                   40.574937\n",
      "collectivelyconscious.net     10.300000\n",
      "nationalreview.com            21.042068\n",
      "naturalnews.com               30.463349\n",
      "strategic-culture.org         30.211809\n",
      "undergroundhealth.com         18.028756\n",
      "unz.com                       21.550986\n",
      "vdare.com                     27.848115\n",
      "wallstreetonparade.com        48.238029\n",
      "washingtonexaminer.com        26.153341\n",
      "washingtonsblog.com          115.278261\n",
      "willyloman.wordpress.com      25.775310\n",
      "www.newsmax.com               23.411909\n",
      "Name: lix, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Group the DataFrame by 'domain' and calculate the average LIX for each group\n",
    "average_lix_by_domain = df.groupby('domain')['lix'].mean()\n",
    "\n",
    "# Find the domain with the highest average LIX\n",
    "highest_lix_domain = average_lix_by_domain.idxmax()\n",
    "highest_lix_value = average_lix_by_domain.max()\n",
    "\n",
    "# Find the domain with the lowest average LIX\n",
    "lowest_lix_domain = average_lix_by_domain.idxmin()\n",
    "lowest_lix_value = average_lix_by_domain.min()\n",
    "\n",
    "print(\"Domain with the highest average LIX:\")\n",
    "print(\"Domain:\", highest_lix_domain)\n",
    "print(\"Average LIX:\", highest_lix_value)\n",
    "\n",
    "print(\"\\nDomain with the lowest average LIX:\")\n",
    "print(\"Domain:\", lowest_lix_domain)\n",
    "print(\"Average LIX:\", lowest_lix_value)\n",
    "\n",
    "print(\"\\nAverage LIX by domain:\")\n",
    "print(average_lix_by_domain)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
