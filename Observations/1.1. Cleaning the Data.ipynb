{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def clean_text(element):\n",
    "    \"\"\"cleans raw data using re.sub() to remove double newlines, space, and tabs. Also replace dates, emails, urls, and numbers\"\"\"\n",
    "    text = str(element).lower()\n",
    "    \n",
    "    num_pattern = re.compile(r\"(\\d+)\")\n",
    "    date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b') #using the dd-mm-yyyy format\n",
    "    date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b')\n",
    "    email_pattern = re.compile(r'\\b[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,}\\b')\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(date_pattern, \"<DATE>\", text)\n",
    "    text = re.sub(email_pattern, \"<EMAIL>\", text)\n",
    "    text = re.sub(url_pattern, \"<URL>\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(num_pattern, \"<NUM>\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unique_words(text: list, n: int):\n",
    "    \"\"\"returns a list of the n most used words in the list\"\"\"\n",
    "    words = {}\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "    \n",
    "    #sorting the dictionary into a list of tuples by word count in descending order\n",
    "    return (sorted(words.items(), key = lambda x:x[1], reverse = True)[0:n])\n",
    "\n",
    "def remove_stopwords(unfiltered_text: list):\n",
    "    \"\"\"remove stopwords from list of strings\"\"\"\n",
    "    filtered_words = []\n",
    "    for word in unfiltered_text: \n",
    "        if word not in stopwords.words('english'): filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "def word_vocabulary_analasis(text: list):\n",
    "    \"\"\"print reduction rate of vocabulary of tokenzied data before and after removing stopwords and stemming\"\"\"\n",
    "    \n",
    "    unique_words = set(text)\n",
    "    stemmed_words = set([PorterStemmer().stem(word) for word in unique_words])\n",
    "    stop_words = remove_stopwords(unique_words)\n",
    "    stop_stemmed_words = remove_stopwords(stemmed_words)\n",
    "    \n",
    "    len_unique_words = len(unique_words)\n",
    "    len_stemmed_words = len(stemmed_words)\n",
    "    len_stop_words = len(stop_words)\n",
    "    len_stop_stemmed_words = len(stop_stemmed_words)\n",
    "    \n",
    "    #printing reduction rate table\n",
    "    print(tabulate([['stemming', 100 * (len_unique_words - len_stemmed_words) / len_unique_words], \n",
    "                    ['removing stopwords', 100 * (len_unique_words - len_stop_words) / len_unique_words], \n",
    "                    ['stemming and removing stopwords', 100 * (len_unique_words - len_stop_stemmed_words) / len_unique_words]], \n",
    "                   headers=['data cleaning type', 'reduction rate of vocabularyin percent'], tablefmt='orgtbl'))\n",
    "\n",
    "def stemming_data(text: str):\n",
    "    \"\"\"returns stemmed data where stopwords are removed from string\"\"\"\n",
    "    unique_words = text.split()\n",
    "    stemmed_words = [PorterStemmer().stem(word) for word in unique_words]\n",
    "    return remove_stopwords(stemmed_words) \n",
    "\n",
    "raw_data_fake_news = pd.read_csv(\"995K_.csv\", dtype={0: str, 1: str})\n",
    "# Selecting the first 10,000 rows\n",
    "raw_data_fake_news = raw_data_fake_news.head(10000)\n",
    "\n",
    "tokenized_data_fake_news = raw_data_fake_news.map(clean_text)\n",
    "\n",
    "list_of_words_content = [\"content\"]\n",
    "for text in tokenized_data_fake_news[\"content\"].tolist():\n",
    "    for word in text.split():\n",
    "        list_of_words_content.append(word)\n",
    "\n",
    "word_vocabulary_analasis(list_of_words_content)\n",
    "\n",
    "stemmed_data_fake_news = tokenized_data_fake_news\n",
    "\n",
    "print(stemmed_data_fake_news)\n",
    "\n",
    "nan_rows = tokenized_data_fake_news[tokenized_data_fake_news[\"authors\"] == \"nan\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def clean_text(element):\n",
    "    \"\"\"cleans raw data using re.sub() to remove double newlines, space, and tabs. Also replace dates, emails, urls, and numbers\"\"\"\n",
    "    text = str(element).lower()\n",
    "    \n",
    "    num_pattern = re.compile(r\"(\\d+)\")\n",
    "    date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b') #using the dd-mm-yyyy format\n",
    "    date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b')\n",
    "    email_pattern = re.compile(r'\\b[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,}\\b')\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(date_pattern, \"<DATE>\", text)\n",
    "    text = re.sub(email_pattern, \"<EMAIL>\", text)\n",
    "    text = re.sub(url_pattern, \"<URL>\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(num_pattern, \"<NUM>\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unique_words(text: list, n: int):\n",
    "    \"\"\"returns a list of the n most used words in the list\"\"\"\n",
    "    words = {}\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "    \n",
    "    #sorting the dictionary into a list of tuples by word count in descending order\n",
    "    return (sorted(words.items(), key = lambda x:x[1], reverse = True)[0:n])\n",
    "\n",
    "def remove_stopwords(unfiltered_text: list):\n",
    "    \"\"\"remove stopwords from list of strings\"\"\"\n",
    "    filtered_words = []\n",
    "    for word in unfiltered_text: \n",
    "        if word not in stopwords.words('english'): filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "def word_vocabulary_analasis(text: list):\n",
    "    \"\"\"print reduction rate of vocabulary of tokenzied data before and after removing stopwords and stemming\"\"\"\n",
    "    \n",
    "    unique_words = set(text)\n",
    "    stemmed_words = set([PorterStemmer().stem(word) for word in unique_words])\n",
    "    stop_words = remove_stopwords(unique_words)\n",
    "    stop_stemmed_words = remove_stopwords(stemmed_words)\n",
    "    \n",
    "    len_unique_words = len(unique_words)\n",
    "    len_stemmed_words = len(stemmed_words)\n",
    "    len_stop_words = len(stop_words)\n",
    "    len_stop_stemmed_words = len(stop_stemmed_words)\n",
    "    \n",
    "    #printing reduction rate table\n",
    "    print(tabulate([['stemming', 100 * (len_unique_words - len_stemmed_words) / len_unique_words], \n",
    "                    ['removing stopwords', 100 * (len_unique_words - len_stop_words) / len_unique_words], \n",
    "                    ['stemming and removing stopwords', 100 * (len_unique_words - len_stop_stemmed_words) / len_unique_words]], \n",
    "                   headers=['data cleaning type', 'reduction rate of vocabularyin percent'], tablefmt='orgtbl'))\n",
    "\n",
    "def stemming_data(text: str):\n",
    "    \"\"\"returns stemmed data where stopwords are removed from string\"\"\"\n",
    "    unique_words = text.split()\n",
    "    stemmed_words = [PorterStemmer().stem(word) for word in unique_words]\n",
    "    return remove_stopwords(stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data_fake_news = pd.read_csv(\"995K_.csv\", dtype={0: str, 1: str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "tokenized_data_fake_news = raw_data_fake_news.map(clean_text)\n",
    "\n",
    "#doing a word analasis and getting the stemmed data without stopwords\n",
    "list_of_words_content = [\"content\"]\n",
    "for text in tokenized_data_fake_news[\"content\"].tolist():\n",
    "    for word in text.split():\n",
    "        list_of_words_content.append(word)\n",
    "\n",
    "word_vocabulary_analasis(list_of_words_content)\n",
    "\n",
    "#stemming and removing stopwords from the data\n",
    "stemmed_data_fake_news = tokenized_data_fake_news\n",
    "\n",
    "#printing the Dataframe\n",
    "tokenized_data_fake_news\n",
    "\n",
    "nan_rows = tokenized_data_fake_news[tokenized_data_fake_news[\"authors\"] == \"nan\"]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
