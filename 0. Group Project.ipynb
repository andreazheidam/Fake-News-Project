{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Graded Exercise 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 1: Regular expression warmup\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.a.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Match found:\n",
      "DD: 01\n",
      "MM: 02\n",
      "YY: 03\n",
      "IIII: 1234\n",
      "Match found:\n",
      "DD: 04\n",
      "MM: 05\n",
      "YY: 06\n",
      "IIII: 5678\n",
      "Match found:\n",
      "DD: 07\n",
      "MM: 08\n",
      "YY: 09\n",
      "IIII: 9101\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# First, i define the the regular expression that matches any CPR number in the DDMMYYIIII format\n",
    "cpr_pattern = r'(\\d{2})(\\d{2})(\\d{2})(\\d{4})'\n",
    "\n",
    "# Sample CPR numbers\n",
    "cpr_numbers = [\n",
    "    '0102031234',\n",
    "    '0405065678',\n",
    "    '0708099101'\n",
    "]\n",
    "\n",
    "# Now, I apply the regular expression to match each CPR number, containing four groups, such that the DD, MM, YY, and IIII parts can be extracted\n",
    "for cpr in cpr_numbers:\n",
    "    match = re.match(cpr_pattern, cpr)\n",
    "    if match:\n",
    "        dd, mm, yy, iiii = match.groups()\n",
    "        print(\"Match found:\")\n",
    "        print(\"DD:\", dd)\n",
    "        print(\"MM:\", mm)\n",
    "        print(\"YY:\", yy)\n",
    "        print(\"IIII:\", iiii)\n",
    "    else:\n",
    "        print(\"No match found for:\", cpr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 1.b.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Century: 1900\n"
     ]
    }
   ],
   "source": [
    "# First, I write a fucntion that returns the relevant century based on the table information\n",
    "def determine_century(identifier, year):\n",
    "    if 1 <= identifier <= 3999:\n",
    "        return 1900\n",
    "    elif 4000 <= identifier <= 4999:\n",
    "        if 0 <= year <= 36:\n",
    "            return 2000\n",
    "        else:\n",
    "            return 1900\n",
    "    elif 5000 <= identifier <= 8999:\n",
    "        if 0 <= year <= 57:\n",
    "            return 2000\n",
    "        else:\n",
    "            return 1800\n",
    "    elif 9000 <= identifier <= 9999:\n",
    "        if 0 <= year <= 36:\n",
    "            return 2000\n",
    "        else:\n",
    "            return 1900\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "# Then, I test the function and remember that Python interprets leading zeros in decimal integers as octal literals\n",
    "identifier = 1234\n",
    "year = 5\n",
    "century = determine_century(identifier, year)\n",
    "print(\"Century:\", century)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 2: Processing the FakeNewsCorpus data set\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pandas'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# 1. Read the CSV file\u001b[39;00m\n\u001b[1;32m      4\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnews_sample.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pandas'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 1. Read the CSV file\n",
    "data = pd.read_csv('news_sample.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First few rows of the dataset:\n",
      "   Unnamed: 0   id                domain        type  \\\n",
      "0           0  141               awm.com  unreliable   \n",
      "1           1  256     beforeitsnews.com        fake   \n",
      "2           2  700           cnnnext.com  unreliable   \n",
      "3           3  768               awm.com  unreliable   \n",
      "4           4  791  bipartisanreport.com   clickbait   \n",
      "\n",
      "                                                 url  \\\n",
      "0  http://awm.com/church-congregation-brings-gift...   \n",
      "1  http://beforeitsnews.com/awakening-start-here/...   \n",
      "2  http://www.cnnnext.com/video/18526/never-hike-...   \n",
      "3  http://awm.com/elusive-alien-of-the-sea-caught...   \n",
      "4  http://bipartisanreport.com/2018/01/21/trumps-...   \n",
      "\n",
      "                                             content  \\\n",
      "0  Sometimes the power of Christmas will make you...   \n",
      "1  AWAKENING OF 12 STRANDS of DNA â€“ â€œReconnecting...   \n",
      "2  Never Hike Alone: A Friday the 13th Fan Film U...   \n",
      "3  When a rare shark was caught, scientists were ...   \n",
      "4  Donald Trump has the unnerving ability to abil...   \n",
      "\n",
      "                   scraped_at                 inserted_at  \\\n",
      "0  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "1  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "2  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "3  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "4  2018-01-25 16:17:44.789555  2018-02-02 01:19:41.756632   \n",
      "\n",
      "                   updated_at  \\\n",
      "0  2018-02-02 01:19:41.756664   \n",
      "1  2018-02-02 01:19:41.756664   \n",
      "2  2018-02-02 01:19:41.756664   \n",
      "3  2018-02-02 01:19:41.756664   \n",
      "4  2018-02-02 01:19:41.756664   \n",
      "\n",
      "                                               title          authors  \\\n",
      "0  Church Congregation Brings Gift to Waitresses ...      Ruth Harris   \n",
      "1  AWAKENING OF 12 STRANDS of DNA â€“ â€œReconnecting...     Zurich Times   \n",
      "2  Never Hike Alone - A Friday the 13th Fan Film ...              NaN   \n",
      "3  Elusive â€˜Alien Of The Sea â€˜ Caught By Scientis...  Alexander Smith   \n",
      "4  Trumpâ€™s Genius Poll Is Complete & The Results ...  Gloria Christie   \n",
      "\n",
      "   keywords meta_keywords                                   meta_description  \\\n",
      "0       NaN          ['']                                                NaN   \n",
      "1       NaN          ['']                                                NaN   \n",
      "2       NaN          ['']  Never Hike Alone: A Friday the 13th Fan Film  ...   \n",
      "3       NaN          ['']                                                NaN   \n",
      "4       NaN          ['']                                                NaN   \n",
      "\n",
      "  tags  summary  \n",
      "0  NaN      NaN  \n",
      "1  NaN      NaN  \n",
      "2  NaN      NaN  \n",
      "3  NaN      NaN  \n",
      "4  NaN      NaN  \n"
     ]
    }
   ],
   "source": [
    "# 2. Manually inspect the data to get an idea of potential problems of the data structure and representation that need to be fixed.\n",
    "\n",
    "# 2.a. View the first few rows\n",
    "print(\"First few rows of the dataset:\")\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- The dataset contains the columns: 'id', 'domain', 'type', 'url', 'content', 'scraped\\_at', 'inserted\\_at', 'updated\\_at', 'title', 'authors', 'keywords', 'meta\\_keywords', 'meta\\_description', 'tags', and 'summary'.\n",
    "- Strings:  'domain', 'type', 'url', 'content', 'title', 'authors', 'meta\\_keywords', 'meta\\_description', 'tags', 'summary'.\n",
    "- Datetime information: 'scraped\\_at', 'inserted\\_at', 'updated\\_at'.\n",
    "- There are missing values in 'authors', 'keywords', 'meta\\_keywords', 'meta\\_description', 'tags', and 'summary'.\n",
    "- **Content:** Main text content of the articles, which might be the focus of any analysis.\n",
    "- **Title:** The titles of the articles.\n",
    "- **Type:** Type of the article: 'unreliable', 'fake' or 'clickbait'.\n",
    "- **URL**: Web adresses of the articles \\-\\-&gt; reference.\n",
    "- **Metadata:** 'Keywords', 'meta\\_keywords', 'meta\\_description',  'tags', and 'summary' describes metadata in relation to the artices \\(some have missing values\\).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Column Names:\n",
      "Index(['Unnamed: 0', 'id', 'domain', 'type', 'url', 'content', 'scraped_at',\n",
      "       'inserted_at', 'updated_at', 'title', 'authors', 'keywords',\n",
      "       'meta_keywords', 'meta_description', 'tags', 'summary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# 2.b. Check column names\n",
    "print(\"\\nColumn Names:\")\n",
    "print(data.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "1. 'unnamed 0' index or identifier column.\n",
    "2. 'id' associated the specific article.\n",
    "3. 'domain' of the website from which the article comes from\n",
    "4. 'type' of the article, such as 'unreliable', 'fake', or 'clickbait'\n",
    "5. 'url'of the article\n",
    "6. 'content' of the article\n",
    "7. 'scraped\\_at': Date and time when scraped\n",
    "8. 'inserted\\_at': Date and time when the article was inserted into the dataset\n",
    "9. 'updated\\_at': Date and time when the article was last updated in the dataset\n",
    "10. 'title': Title of the article\n",
    "11. 'authors' of the article\n",
    "12. 'keywords' associated with the article \\(may contain missing values\\)\n",
    "13. 'meta\\_keywords' associated with the article \\(may contain missing values\\)\n",
    "14. 'meta\\_description' of the article \\(may contain missing values\\)\n",
    "15. 'tags' associated with the article \\(may contain missing values\\)\n",
    "16. 'summary' of the article \\(may contain missing values\\)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data Types:\n",
      "Unnamed: 0            int64\n",
      "id                    int64\n",
      "domain               object\n",
      "type                 object\n",
      "url                  object\n",
      "content              object\n",
      "scraped_at           object\n",
      "inserted_at          object\n",
      "updated_at           object\n",
      "title                object\n",
      "authors              object\n",
      "keywords            float64\n",
      "meta_keywords        object\n",
      "meta_description     object\n",
      "tags                 object\n",
      "summary             float64\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2.c. Check data types\n",
    "print(\"\\nData Types:\")\n",
    "print(data.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 'scraped\\_at', 'inserted\\_at', and 'updated\\_at' should be converted from string to datatime data type \\-\\-&gt; better handling and analysis.\n",
    "- 'keywords' and 'summary' contain floating\\-point numbers \\-\\-&gt; need further information in order to determine whether it is correct data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Missing Values:\n",
      "Unnamed: 0            0\n",
      "id                    0\n",
      "domain                0\n",
      "type                 12\n",
      "url                   0\n",
      "content               0\n",
      "scraped_at            0\n",
      "inserted_at           0\n",
      "updated_at            0\n",
      "title                 0\n",
      "authors              80\n",
      "keywords            250\n",
      "meta_keywords         0\n",
      "meta_description    196\n",
      "tags                223\n",
      "summary             250\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2.d. Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- For 'type', 'authors', 'meta\\_description', 'tags', and 'meta\\_keywords' the number of missing values are small compared to the whole data set, why it might be okay to remove rows with missing values.\n",
    "- For 'keywords' and 'summary' a large number of values are missing, why it might be necessary to find the reason of this in order to find another way to handle this.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of duplicate rows: 0\n"
     ]
    }
   ],
   "source": [
    "# 2.e. Check for duplicate rows\n",
    "print(\"\\nNumber of duplicate rows:\", data.duplicated().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sample of text data:\n",
      "0    Sometimes the power of Christmas will make you...\n",
      "1    AWAKENING OF 12 STRANDS of DNA â€“ â€œReconnecting...\n",
      "2    Never Hike Alone: A Friday the 13th Fan Film U...\n",
      "3    When a rare shark was caught, scientists were ...\n",
      "4    Donald Trump has the unnerving ability to abil...\n",
      "Name: content, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# 2.f. Inspect a sample of text data\n",
    "text_column_name = 'content'\n",
    "print(\"\\nSample of text data:\")\n",
    "print(data[text_column_name].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 'Content' contains divers topics.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Distribution of the 'type' variable:\n",
      "type\n",
      "fake          155\n",
      "conspiracy     31\n",
      "political      23\n",
      "unreliable      6\n",
      "bias            6\n",
      "junksci         6\n",
      "unknown         6\n",
      "reliable        3\n",
      "clickbait       1\n",
      "hate            1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# 2.g. Explore the distribution of categorical variables (e.g., 'label')\n",
    "print(\"\\nDistribution of the 'type' variable:\")\n",
    "print(data['type'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- Large portion of articles categorized as 'fake', 'conspiracy' and 'political'.\n",
    "- Small portion of articles categorized as 'reliable', 'clickbait', and 'hate'\n",
    "- If specific classes are underrepresented, this might cause problems.\n",
    "- 'unknown' and 'junksci' cause misspecification on the categorizing of the article classes.\n",
    "- 'reliable', 'clickbait', and 'hate' might not be well enough represented, meaning no pattern and generalization of these types.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Summary statistics for numeric columns:\n",
      "       Unnamed: 0            id  keywords  summary\n",
      "count  250.000000    250.000000       0.0      0.0\n",
      "mean   124.500000  20241.560000       NaN      NaN\n",
      "std     72.312977  11515.412728       NaN      NaN\n",
      "min      0.000000    141.000000       NaN      NaN\n",
      "25%     62.250000  11033.500000       NaN      NaN\n",
      "50%    124.500000  21065.000000       NaN      NaN\n",
      "75%    186.750000  29073.750000       NaN      NaN\n",
      "max    249.000000  39558.000000       NaN      NaN\n"
     ]
    }
   ],
   "source": [
    "# 2.e. Summary statistics for numeric columns\n",
    "print(\"\\nSummary statistics for numeric columns:\")\n",
    "print(data.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "- 'keywords' and 'summary' contain no non\\-null values \\-\\-&gt; their statistics are not applicable in this context.\n",
    "- 'Unnamed: 0' and 'id' are identifiers \\-\\-&gt; not necessary meaningful numeric features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Text:\n",
      "the meeting is scheduled on <NUM>-<NUM>-<NUM> at <NUM>:<NUM> pm. please send your rsvp to <EMAIL> or visit <URL>\n"
     ]
    }
   ],
   "source": [
    "# Clean the data manually, writing own clean_text() function using regular expressions\n",
    "\n",
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "    # First, convert all words to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Now, replace multiple white spaces, tabs, and new lines with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Replace numbers with <NUM>\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUM>', text)\n",
    "    \n",
    "    # Replacing dates with <DATE> (Choosing one date format present in the data as 'YYYY-MM-DD' format)\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)\n",
    "    \n",
    "    # Replace emails with <EMAIL> (simple pattern)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', text)\n",
    "    \n",
    "    # Replace URLs with <URL> (simple pattern)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Let us test the function\n",
    "raw_text = \"The meeting is scheduled on 2024-02-19 at 2:30 PM. Please send your RSVP to john@example.com or visit https://example.com\"\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(\"Cleaned Text:\")\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                  \r\n",
      "â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•¦â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\r\n",
      "â•‘   âš    NETWORK DISABLED  âš     â•‘    âš    NO INTERNET ACCESS  âš     â•‘\r\n",
      "â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•©â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\r\n",
      "â•‘ This project does not have access to the internet.             â•‘\r\n",
      "â•‘ Add a valid license in order to enable internet access.        â•‘\r\n",
      "â•‘ Otherwise, you cannot pull from a Git repository, use cURL,    â•‘\r\n",
      "â•‘ wget, download Python packages from PyPI, etc.                 â•‘\r\n",
      "â•‘ Requests to load data will fail or hang indefinitely.          â•‘\r\n",
      "â•‘                                                                â•‘\r\n",
      "â•‘            https://cocalc.com/store/site-license               â•‘\r\n",
      "â•‘                                                                â•‘\r\n",
      "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢\r\n",
      "â•‘ Attempting to install software? It might already be available! â•‘\r\n",
      "â•‘                https://cocalc.com/software                     â•‘\r\n",
      "â•Ÿâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•¢\r\n",
      "â•‘ However, you can become admin in a ğ—–ğ—¢ğ— ğ—£ğ—¨ğ—§ğ—˜ ğ—¦ğ—˜ğ—¥ğ—©ğ—˜ğ—¥!             â•‘\r\n",
      "â•‘ Learn more here:    https://doc.cocalc.com/compute_server.html â•‘\r\n",
      "â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\r\n",
      "                                                                  \r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\r\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa162834f40>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/clean-text/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa162835210>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/clean-text/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa16282ff40>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/clean-text/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa1628353c0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/clean-text/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ConnectTimeoutError(<pip._vendor.urllib3.connection.HTTPSConnection object at 0x7fa1628340d0>, 'Connection to pypi.org timed out. (connect timeout=15)')': /simple/clean-text/\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement clean_text (from versions: none)\u001b[0m\u001b[31m\r\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for clean_text\u001b[0m\u001b[31m\r\n",
      "\u001b[0m"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'clean' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1360/822356938.py\u001b[0m in \u001b[0;36m<cell line: 20>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Let us test the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0mraw_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"The meeting is scheduled on 2024-02-19 at 2:30 PM. Please send your RSVP to john@example.com or visit https://example.com\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mraw_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Cleaned Text:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcleaned_text\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_1360/822356938.py\u001b[0m in \u001b[0;36mclean_text\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Clean the text using the specified cleaning steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mcleaned_text\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcleaning_steps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mcleaned_text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'clean' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "def clean_text(text):\n",
    "    # Define the cleaning steps\n",
    "    cleaning_steps = {\n",
    "        'lowercase': True,\n",
    "        'replace_with_digit': '<NUM>',\n",
    "        'replace_with_url': '<URL>',\n",
    "        'replace_with_email': '<EMAIL>',\n",
    "        'replace_with_punct': '',  # Remove punctuation\n",
    "        'replace_with_whitespace': ' ',  # Replace multiple whitespaces with single space\n",
    "    }\n",
    "    \n",
    "    # Clean the text using the specified cleaning steps\n",
    "    cleaned_text = clean(text, **cleaning_steps)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "# Let us test the function\n",
    "raw_text = \"The meeting is scheduled on 2024-02-19 at 2:30 PM. Please send your RSVP to john@example.com or visit https://example.com\"\n",
    "cleaned_text = clean_text(raw_text)\n",
    "print(\"Cleaned Text:\")\n",
    "print(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Part 3: Descriptive frequency analysis of the data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique words in raw text: 19435\n",
      "Number of unique words in cleaned text: 15448\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Function to tokenize text into words\n",
    "def tokenize(text):\n",
    "    # Split the text into words based on non-alphanumeric characters\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return words\n",
    "\n",
    "# Function to calculate the number of unique words\n",
    "def count_unique_words(text):\n",
    "    # Tokenize the text\n",
    "    words = tokenize(text)\n",
    "    # Count the number of unique words\n",
    "    unique_words = set(words)\n",
    "    return len(unique_words)\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # First, convert all words to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Now, replace multiple white spaces, tabs, and new lines with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Replace numbers with <NUM>\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUM>', text)\n",
    "    \n",
    "    # Replacing dates with <DATE> (Choosing one date format present in the data as 'YYYY-MM-DD' format)\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)\n",
    "    \n",
    "    # Replace emails with <EMAIL> (simple pattern)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', text)\n",
    "    \n",
    "    # Replace URLs with <URL> (simple pattern)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('news_sample.csv')\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "data['cleaned_text'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# Concatenate all cleaned text into one string\n",
    "cleaned_text = ' '.join(data['cleaned_text'])\n",
    "\n",
    "# Calculate the number of unique words in raw text\n",
    "num_unique_words_raw = count_unique_words(' '.join(data['content']))\n",
    "\n",
    "# Calculate the number of unique words in cleaned text\n",
    "num_unique_words_cleaned = count_unique_words(cleaned_text)\n",
    "\n",
    "print(\"Number of unique words in raw text:\", num_unique_words_raw)\n",
    "print(\"Number of unique words in cleaned text:\", num_unique_words_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### 3.2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to preprocess the text\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mre\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Function to preprocess the text\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:988\u001b[0m, in \u001b[0;36mPyDBFrame.trace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    986\u001b[0m \u001b[38;5;66;03m# if thread has a suspend flag, we suspend with a busy wait\u001b[39;00m\n\u001b[1;32m    987\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m info\u001b[38;5;241m.\u001b[39mpydev_state \u001b[38;5;241m==\u001b[39m STATE_SUSPEND:\n\u001b[0;32m--> 988\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrace_dispatch\n\u001b[1;32m    990\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:165\u001b[0m, in \u001b[0;36mPyDBFrame.do_wait_suspend\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_wait_suspend\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 165\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_args\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to preprocess the text\n",
    "def preprocess_text(text):\n",
    "    # First, convert all words to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Now, replace multiple white spaces, tabs, and new lines with a single space\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    \n",
    "    # Replace numbers with <NUM>\n",
    "    text = re.sub(r'\\b\\d+\\b', '<NUM>', text)\n",
    "    \n",
    "    # Replacing dates with <DATE> (Choosing one date format present in the data as 'YYYY-MM-DD' format)\n",
    "    text = re.sub(r'\\b\\d{4}-\\d{2}-\\d{2}\\b', '<DATE>', text)\n",
    "    \n",
    "    # Replace emails with <EMAIL> (simple pattern)\n",
    "    text = re.sub(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b', '<EMAIL>', text)\n",
    "    \n",
    "    # Replace URLs with <URL> (simple pattern)\n",
    "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '<URL>', text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Function to tokenize text into words\n",
    "def tokenize(text):\n",
    "    # Split the text into words based on space\n",
    "    words = text.split()\n",
    "    return words\n",
    "\n",
    "# Function to count word frequencies\n",
    "def count_word_frequencies(text):\n",
    "    words = tokenize(text)\n",
    "    word_freq = {}\n",
    "    for word in words:\n",
    "        word_freq[word] = word_freq.get(word, 0) + 1\n",
    "    return word_freq\n",
    "\n",
    "# Function to plot barplot\n",
    "def plot_barplot(word_freq):\n",
    "    sorted_word_freq = sorted(word_freq.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_words = [word[0] for word in sorted_word_freq[:50]]\n",
    "    frequencies = [word[1] for word in sorted_word_freq[:50]]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(top_words, frequencies, color='skyblue')\n",
    "    plt.xlabel('Words')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Top 50 Most Frequent Words')\n",
    "    plt.xticks(rotation=90)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Load the dataset\n",
    "data = pd.read_csv('news_sample.csv')\n",
    "\n",
    "# Apply preprocessing to the text column\n",
    "data['cleaned_text'] = data['content'].apply(preprocess_text)\n",
    "\n",
    "# Concatenate all cleaned text into one string\n",
    "cleaned_text = ' '.join(data['cleaned_text'])\n",
    "\n",
    "# Count word frequencies\n",
    "word_freq = count_word_frequencies(cleaned_text)\n",
    "\n",
    "# Plot the barplot\n",
    "plot_barplot(word_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "argv": [
    "/usr/bin/python3",
    "-m",
    "ipykernel",
    "--HistoryManager.enabled=False",
    "--matplotlib=inline",
    "-c",
    "%config InlineBackend.figure_formats = set(['retina'])\nimport matplotlib; matplotlib.rcParams['figure.figsize'] = (12, 7)",
    "-f",
    "{connection_file}"
   ],
   "display_name": "Python 3 (system-wide)",
   "env": {},
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3",
   "resource_dir": "/ext/jupyter/kernels/python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
