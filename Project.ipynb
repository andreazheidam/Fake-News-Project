{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Importing and cleaning the a sample of the FakeNewsCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import requests\n",
    "from tabulate import tabulate\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"cleans raw data using re.sub() to remove double newlines, space, and tabs. Also replace dates, emails, urls, and numbers\"\"\"\n",
    "    text = text.lower()\n",
    "    \n",
    "    num_pattern = re.compile(r\"(\\d+)\")\n",
    "    date_pattern = re.compile(r\"((\\d{2})-(\\d{2})-(\\d{4}))\") #using the dd-mm-yyyy format\n",
    "    email_pattern = re.compile(r\"(([\\w\\-_.]*)(@\\w+)(.com))\")\n",
    "    url_pattern = re.compile(r\"((https:\\/\\/www\\.)([a-zA-Z0-9]*)(\\.com))\")\n",
    "    \n",
    "    text = re.sub(\" +\", \" \", text)\n",
    "    text = re.sub(\"\\t+\", \"\\t\", text)\n",
    "    text = re.sub(\"\\n+\", \"\\n\", text)\n",
    "    text = re.sub(date_pattern, \"<DATE>\", text)\n",
    "    text = re.sub(email_pattern, \"<EMAIL>\", text)\n",
    "    text = re.sub(url_pattern, \"<URL>\", text)\n",
    "    text = re.sub(num_pattern, \"<NUM>\", text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "def unique_words_plot(text: list, n: int):\n",
    "    \"\"\"returns a list of the n most used words in the list\"\"\"\n",
    "    words = {}\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "    \n",
    "    #sorting the dictionary into a list of tuples by word count in descending order\n",
    "    return (sorted(words.items(), key = lambda x:x[1], reverse = True)[0:n])\n",
    "\n",
    "def word_vocabulary_analasis(text: list):\n",
    "    \"\"\"returns reduction rate of vocabulary of tokenzied data before and after removing stopwords and stemming\"\"\"\n",
    "    \n",
    "    def remove_stopwords(unfiltered_text: list):\n",
    "        filtered_words = []\n",
    "        for word in unfiltered_text: \n",
    "            if word not in stopwords.words('english'): filtered_words.append(word)\n",
    "        return filtered_words\n",
    "    \n",
    "    unique_words = set(text)\n",
    "    stemmed_words = set([PorterStemmer().stem(word) for word in unique_words])\n",
    "    stop_words = remove_stopwords(unique_words)\n",
    "    stop_stemmed_words = remove_stopwords(stemmed_words)\n",
    "    \n",
    "    len_unique_words = len(unique_words)\n",
    "    len_stemmed_words = len(stemmed_words)\n",
    "    len_stop_words = len(stop_words)\n",
    "    len_stop_stemmed_words = len(stop_stemmed_words)\n",
    "    \n",
    "    #printing reduction rate table\n",
    "    print(tabulate([['stemming', 100 * (len_unique_words - len_stemmed_words) / len_unique_words], \n",
    "                    ['removing stopwords', 100 * (len_unique_words - len_stop_words) / len_unique_words], \n",
    "                    ['stemming and removing stopwords', 100 * (len_unique_words - len_stop_stemmed_words) / len_unique_words]], \n",
    "                   headers=['data cleaning type', 'reduction rate of vocabularyin percent'], tablefmt='orgtbl'))\n",
    "    \n",
    "    return stop_stemmed_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import the data and run our functions. (Remember to run the functions first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| data type                       |   reduction rate of vocabulary |\n",
      "|---------------------------------+--------------------------------|\n",
      "| stemming                        |                      24.2999   |\n",
      "| removing stopwords              |                       0.695249 |\n",
      "| stemming and removing stopwords |                      24.889    |\n"
     ]
    }
   ],
   "source": [
    "#importing the CVS file as raw text\n",
    "raw_data_fake_news = requests.get(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\").text\n",
    "\n",
    "#cleaning raw text\n",
    "cleaned_data_fake_news = clean_text(raw_data_fake_news)\n",
    "\n",
    "#tokenizing raw text\n",
    "tokenized_data_fake_news = nltk.word_tokenize(cleaned_data_fake_news, language=\"english\", preserve_line=True)\n",
    "\n",
    "#doing a word analasis and getting the stemmed data without stopwords\n",
    "stemmed_data_fake_news = word_vocabulary_analasis(tokenized_data_fake_news)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
