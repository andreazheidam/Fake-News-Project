{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data_fake_news = pd.read_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\Data_Copy.csv\", low_memory=False)\n",
    "#raw_data_fake_news = pd.read_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\995,000_rows.csv\", low_memory=False)\n",
    "\n",
    "data_fake_news = raw_data_fake_news.copy()[ #keeping only the relevant collumns\n",
    "    ['domain', 'type', 'content', 'title', 'authors', 'meta_description', 'meta_keywords']].head(1000)\n",
    "\n",
    "data_fake_news.to_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\Data_Copy.csv\", index=False) #save copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "\n",
    "#compile regular expressions\n",
    "num_pattern = re.compile(r\"(\\d+)\")\n",
    "date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b')\n",
    "email_pattern = re.compile(r'\\b[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,}\\b')\n",
    "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"cleans raw data\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(date_pattern, \"<DATE>\", text)\n",
    "    text = re.sub(email_pattern, \"<EMAIL>\", text)\n",
    "    text = re.sub(url_pattern, \"<URL>\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(num_pattern, \"<NUM>\", text)\n",
    "    return text\n",
    "\n",
    "#compile stopwords and initialize stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_no_stopwords_data(text: str):\n",
    "    \"\"\"returns stemmed data where stopwords are removed from string\"\"\"\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning raw text using clean_text function on all elements\n",
    "data_fake_news['content'] = data_fake_news['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming and removing stopwords, while calculating the reduction in vocabulary\n",
    "data_fake_news['content'] = data_fake_news['content'].apply(stemming_no_stopwords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore') #initialize encoder\n",
    "def str_encoder(X: str):\n",
    "    \"\"\"Transform X string-data using OneHotEncoder\"\"\"\n",
    "    return one_hot_encoder.fit_transform(X.values.reshape(-1, 1))\n",
    "\n",
    "def logistic_model(X, y):\n",
    "    \"\"\"Create a logistic model and return accuracy\"\"\"\n",
    "    X_encoded = str_encoder(X)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_encoded, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    model_logistic = LogisticRegression()\n",
    "    model_logistic.fit(X_train, y_train)\n",
    "    return accuracy_score(y_val, model_logistic.predict(X_val))\n",
    "\n",
    "def df_type_binary(df):\n",
    "    \"\"\"Return the dataframe where the types grouped into reliable or fake\"\"\"\n",
    "    reliable_type = {\"reliable\", \"political\", \"clickbait\"}\n",
    "    df[\"type\"] = df[\"type\"].apply(lambda x: x in reliable_type)\n",
    "    return df[~df[\"type\"].isin(['fake', 'satire', 'bias', 'conspiracy', 'junksci'])]\n",
    "\n",
    "def naive_bayes_model(X, y):\n",
    "    \"\"\"Create a classification matrix model and return accuracy\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', MultinomialNB()),\n",
    "        ])\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    return accuracy_score(y_val, text_clf.predict(X_val))\n",
    "\n",
    "def sdc_model(X, y):\n",
    "    \"\"\"Create a classification matrix model and return accuracy\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer()),\n",
    "        ('tfidf', TfidfTransformer(use_idf=False)),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                              alpha=1e-3, random_state=0,\n",
    "                              max_iter=5, tol=None)),\n",
    "        ])\n",
    "    text_clf.fit(X_train, y_train)\n",
    "    return accuracy_score(y_val, text_clf.predict(X_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| model             | X data           |   Accuracy |\n",
      "|-------------------+------------------+------------|\n",
      "| random            | none             |       0.58 |\n",
      "| logistic          | domain           |       0.92 |\n",
      "| logistic          | content          |       0.63 |\n",
      "| logistic          | title            |       0.62 |\n",
      "| logistic          | meta description |       0.58 |\n",
      "| logistic          | meta keywords    |       0.74 |\n",
      "| pipeline          | content          |       0.67 |\n",
      "| pipeline improved | content          |       0.73 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maher\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:723: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "#prepareing y data\n",
    "binary_fake_news = df_type_binary(data_fake_news.copy())\n",
    "y = binary_fake_news['type']\n",
    "\n",
    "#random guessing from porportion of reliable to fake data\n",
    "fake_count, reliable_count = binary_fake_news['type'].value_counts()\n",
    "random_acc = fake_count / (fake_count + reliable_count)\n",
    "\n",
    "#creating models for different types of X data\n",
    "print(tabulate([\n",
    "    [\"random\", \"none\", random_acc],\n",
    "    [\"logistic\", \"domain\", logistic_model(binary_fake_news['domain'], y)], \n",
    "    [\"logistic\", \"content\", logistic_model(binary_fake_news['content'], y)],\n",
    "    [\"logistic\", \"title\", logistic_model(binary_fake_news['title'], y)],\n",
    "    [\"logistic\", \"meta description\", logistic_model(binary_fake_news['meta_description'], y)],\n",
    "    [\"logistic\", \"meta keywords\", logistic_model(binary_fake_news['meta_keywords'], y)],\n",
    "    [\"pipeline\", \"content\", naive_bayes_model(binary_fake_news['content'], y)],\n",
    "    [\"pipeline improved\", \"content\", sdc_model(binary_fake_news['content'], y)],\n",
    "    ], headers=[\"model\", \"X data\", \"Accuracy\"], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe what importan parameters to use in function\n",
    "\n",
    "\n",
    "BBC with or without"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
