{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "raw_data_fake_news = pd.read_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\Data_Copy.csv\", low_memory=False)\n",
    "#raw_data_fake_news = pd.read_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\995,000_rows.csv\", low_memory=False)\n",
    "\n",
    "data_fake_news = raw_data_fake_news.copy()[ #keeping only the relevant collumns\n",
    "    ['domain', 'type', 'content', 'title', 'authors', 'meta_description', 'meta_keywords']].head(1000)\n",
    "\n",
    "data_fake_news.to_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\Data_Copy.csv\", index=False) #save copy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from tabulate import tabulate\n",
    "\n",
    "#compile regular expressions\n",
    "num_pattern = re.compile(r\"(\\d+)\")\n",
    "date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b')\n",
    "email_pattern = re.compile(r'\\b[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,}\\b')\n",
    "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def clean_text(text: str):\n",
    "    \"\"\"cleans raw data\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(date_pattern, \"<DATE>\", text)\n",
    "    text = re.sub(email_pattern, \"<EMAIL>\", text)\n",
    "    text = re.sub(url_pattern, \"<URL>\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(num_pattern, \"<NUM>\", text)\n",
    "    return text\n",
    "\n",
    "#compile stopwords and initialize stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_no_stopwords_data(text: str):\n",
    "    \"\"\"returns stemmed data where stopwords are removed from string\"\"\"\n",
    "    return ' '.join([stemmer.stem(word) for word in text.split() if word not in stop_words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning raw text using clean_text function on all elements\n",
    "data_fake_news['content'] = data_fake_news['content'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming and removing stopwords, while calculating the reduction in vocabulary\n",
    "data_fake_news['content'] = data_fake_news['content'].apply(stemming_no_stopwords_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def create_logistic_model(X, y):\n",
    "    \"\"\"Create a logistic model and return accuracy\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    model_logistic = LogisticRegression()\n",
    "    model_logistic.fit(X_train, y_train)  # Train logistic model\n",
    "    return accuracy_score(y_val, model_logistic.predict(X_val))  # Test accuracy\n",
    "\n",
    "def df_type_binary(df):\n",
    "    \"\"\"Return the dataframe where the types grouped into reliable or fake\"\"\"\n",
    "    reliable_type = {\"reliable\", \"political\", \"clickbait\"}\n",
    "    df[\"type\"] = df[\"type\"].apply(lambda x: x in reliable_type)\n",
    "    return df[~df[\"type\"].isin(['fake', 'satire', 'bias', 'conspiracy', 'junksci'])]\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore') #initialize encoder\n",
    "def transform_str_encoder(X: str):\n",
    "    \"\"\"Transform X string-data using OneHotEncoder\"\"\"\n",
    "    return one_hot_encoder.fit_transform(X.values.reshape(-1, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| X data                     |   Accuracy |\n",
      "|----------------------------+------------|\n",
      "| randomly quessing          |       0.58 |\n",
      "| domain                     |       0.92 |\n",
      "| content                    |       0.63 |\n",
      "| title                      |       0.62 |\n",
      "| number of unique words     |       0.63 |\n",
      "| percentage of unique words |       0.58 |\n",
      "| meta description           |       0.58 |\n",
      "| meta keywords              |       0.74 |\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Maher\\AppData\\Local\\Temp\\ipykernel_27348\\4244705244.py:16: FutureWarning: Series.__getitem__ treating keys as positions is deprecated. In a future version, integer keys will always be treated as labels (consistent with DataFrame behavior). To access a value by position, use `ser.iloc[pos]`\n",
      "  random_acc = binary_fake_news['type'].value_counts()[0] / (fake_count + reliable_count)\n"
     ]
    }
   ],
   "source": [
    "#prepareing y data\n",
    "binary_fake_news = df_type_binary(data_fake_news.copy())\n",
    "y = binary_fake_news['type']\n",
    "\n",
    "\n",
    "#prepareing X data\n",
    "X_domain = transform_str_encoder(binary_fake_news['domain'])\n",
    "X_content = transform_str_encoder(binary_fake_news['content'])\n",
    "X_title = transform_str_encoder(binary_fake_news['title'])\n",
    "X_meta_desc = transform_str_encoder(binary_fake_news['meta_description'])\n",
    "X_meta_key = transform_str_encoder(binary_fake_news['meta_keywords'])\n",
    "\n",
    "#create pipeline\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('content', one_hot_encoder, ['content']),\n",
    "        ('title', one_hot_encoder, ['title']),\n",
    "        ('meta_desc', one_hot_encoder, ['meta_description']),\n",
    "        ('meta_key', one_hot_encoder, ['meta_keywords'])\n",
    "    ])\n",
    "\n",
    "# Define pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline.fit(binary_fake_news[['content', 'title', 'meta_description', 'meta_keywords']], y)\n",
    "\n",
    "\n",
    "#random guessing from porportion of reliable to fake data\n",
    "fake_count, reliable_count = binary_fake_news['type'].value_counts()\n",
    "random_acc = fake_count / (fake_count + reliable_count)\n",
    "\n",
    "#creating logistic models for different types of X data\n",
    "print(tabulate([\n",
    "    [\"randomly quessing\", random_acc],\n",
    "    [\"domain\", create_logistic_model(X_domain, y)], \n",
    "    [\"content\", create_logistic_model(X_content, y)],\n",
    "    [\"title\", create_logistic_model(X_title, y)],\n",
    "    [\"meta description\", create_logistic_model(X_meta_desc, y)],\n",
    "    [\"meta keywords\", create_logistic_model(X_meta_key, y)]\n",
    "    ], headers=[\"X data\", \"Accuracy\"], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "describe what importan parameters to use in function\n",
    "\n",
    "\n",
    "BBC with or without"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
