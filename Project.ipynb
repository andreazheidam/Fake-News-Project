{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "new_copy = False #run the data cleaning and create a new CSV file\n",
    "\n",
    "if new_copy: \n",
    "    raw_data_fake_news = pd.read_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\995,000_rows.csv\", low_memory=False)\n",
    "    data_fake_news = raw_data_fake_news.copy()[['domain', 'type', 'content', 'title', 'authors', 'meta_description', 'meta_keywords']]\n",
    "    \n",
    "else: data_fake_news = pd.read_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\Data_Copy.csv\", low_memory=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "#compile regular expressions\n",
    "num_pattern = re.compile(r\"(\\d+)\")\n",
    "date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b')\n",
    "email_pattern = re.compile(r'\\b[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,}\\b')\n",
    "url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "\n",
    "def tokenize_text(text: str):\n",
    "    \"\"\"cleans raw data\"\"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(date_pattern, \"<DATE>\", text)\n",
    "    text = re.sub(email_pattern, \"<EMAIL>\", text)\n",
    "    text = re.sub(url_pattern, \"<URL>\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(num_pattern, \"<NUM>\", text)\n",
    "    return text\n",
    "\n",
    "#compile stopwords and initialize stemmer\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def stemming_no_stopwords(text: str):\n",
    "    \"\"\"returns stemmed data where stopwords are removed from string\"\"\"\n",
    "    return ' '.join(stemmer.stem(word) for word in text.split() if word not in stop_words)\n",
    "\n",
    "def df_type_binary(df):\n",
    "    \"\"\"Return the dataframe where the types grouped into reliable or fake\"\"\"\n",
    "    reliable_type = {\"reliable\", \"political\", \"clickbait\"}\n",
    "    df[\"type\"] = df[\"type\"].apply(lambda x: x in reliable_type)\n",
    "    return df[~df[\"type\"].isin(['fake', 'satire', 'bias', 'conspiracy', 'junksci'])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning raw text using clean_text function on all elements\n",
    "if new_copy: data_fake_news['content'] = data_fake_news['content'].apply(tokenize_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemming and removing stopwords, while calculating the reduction in vocabulary\n",
    "if new_copy: \n",
    "    data_fake_news['content'] = data_fake_news['content'].apply(stemming_no_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save backup copy\n",
    "if new_copy: \n",
    "    data_fake_news.to_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\Data_Copy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "#Importing BBC data\n",
    "with open(\"C:\\\\Users\\\\Maher\\\\Documents\\\\GitHub\\\\Fake-News-Project\\\\BBC.json\", 'r') as file:\n",
    "    BBC_data = [next(iter(article.items()))[1][2] for article in json.load(file) if not next(iter(article.items()))[1][2] == None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing data for model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove empty rows and create two type classes: False (fake) and True (reliable)\n",
    "complete_fake_news = data_fake_news.copy()\n",
    "complete_fake_news[['type', 'content']] = complete_fake_news[['type', 'content']].dropna()\n",
    "complete_fake_news = df_type_binary(complete_fake_news)\n",
    "\n",
    "#saving 10% of articles (excluding BBC) for X_test\n",
    "complete_len = len(complete_fake_news)\n",
    "sample_len = round(complete_len * 0.9)\n",
    "sample_fake_news = complete_fake_news.copy().head(sample_len)\n",
    "sample_fake_news = sample_fake_news.sample(frac=1).reset_index(drop=True) #shuffleling data\n",
    "\n",
    "X_extend = pd.concat([pd.Series(BBC_data).apply(tokenize_text).apply(stemming_no_stopwords), \n",
    "                      sample_fake_news['content'].copy()])\n",
    "y_extend = pd.concat([pd.Series([True] * len(BBC_data)), \n",
    "                      sample_fake_news['type'].copy()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing data on simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "\n",
    "def str_encoder(X: str):\n",
    "    \"\"\"Transform X string-data using OneHotEncoder\"\"\"\n",
    "    return one_hot_encoder.fit_transform(X.values.reshape(-1, 1))\n",
    "\n",
    "def logistic_model(X, y):\n",
    "    \"\"\"Create a logistic model and return accuracy\"\"\"\n",
    "    X_encoded = str_encoder(X)\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X_encoded, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    model = LogisticRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    return [f1_score(y_val, predictions), accuracy_score(y_val, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "#creating logistic models for different types of X data\n",
    "print(tabulate([\n",
    "    [\"domain\"] + logistic_model(sample_fake_news['domain'], sample_fake_news['type']),\n",
    "    [\"title\"] + logistic_model(sample_fake_news['title'], sample_fake_news['type']),\n",
    "    [\"author\"] + logistic_model(sample_fake_news['authors'], sample_fake_news['type']),\n",
    "    [\"meta description\"] + logistic_model(sample_fake_news['meta_description'], sample_fake_news['type']),\n",
    "    [\"meta keywords\"] + logistic_model(sample_fake_news['meta_keywords'], sample_fake_news['type']),\n",
    "    [\"content\"] + logistic_model(sample_fake_news['content'], sample_fake_news['type']),\n",
    "    [\"content BBC extended\"] + logistic_model(X_extend, y_extend),\n",
    "    ], headers=[\"Train data\", \"f1\", \"Accuracy\"], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating three advanced models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "def naive_bayes_model(X, y):\n",
    "    \"\"\"Create a naive bayes model and return accuracy\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    model = Pipeline([\n",
    "        ('Vectorizer', CountVectorizer()),\n",
    "        ('Transformer', TfidfTransformer(use_idf=False)),\n",
    "        ('Model', MultinomialNB()),\n",
    "        ])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    return [f1_score(y_val, predictions), accuracy_score(y_val, predictions)]\n",
    "\n",
    "def stochastic_gradient_descent_model(X, y):\n",
    "    \"\"\"Create a classification matrix model and return accuracy\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    model = Pipeline([\n",
    "        ('Vectorizer', CountVectorizer()),\n",
    "        ('Transformer', TfidfTransformer(use_idf=False)),\n",
    "        ('Model', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                              alpha=1e-4, random_state=0)),\n",
    "        ])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    return [f1_score(y_val, predictions), accuracy_score(y_val, predictions)]\n",
    "\n",
    "def stochastic_gradient_descent_hash_model(X, y):\n",
    "    \"\"\"Create a classification matrix model and return accuracy\"\"\"\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, train_size=0.9, stratify=y, random_state=0)\n",
    "    model = Pipeline([\n",
    "        ('Vectorizer', HashingVectorizer()),\n",
    "        ('Transformer', TfidfTransformer(use_idf=False)),\n",
    "        ('Model', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                              alpha=1e-4, random_state=0)),\n",
    "        ])\n",
    "    model.fit(X_train, y_train)\n",
    "    predictions = model.predict(X_val)\n",
    "    return [f1_score(y_val, predictions), accuracy_score(y_val, predictions)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#printing accuracy of advanced models using the BBC extended data\n",
    "print(tabulate([\n",
    "    [\"Naive Bayes with count vectorizer\"] + naive_bayes_model(sample_fake_news['content'], sample_fake_news['type']),\n",
    "    [\"SDC with count vectorizer\"] + stochastic_gradient_descent_model(sample_fake_news['content'], sample_fake_news['type']),\n",
    "    [\"SDC with hashing vectorizer\"] + stochastic_gradient_descent_hash_model(sample_fake_news['content'], sample_fake_news['type']),\n",
    "    ], headers=[\"Model\", \"f1\", \"Accuracy\"], tablefmt='orgtbl'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating function for simple model\n",
    "simple_model = LogisticRegression()\n",
    "one_hot_encoder = OneHotEncoder(handle_unknown='ignore')\n",
    "simple_model.fit(one_hot_encoder.fit_transform(sample_fake_news['content'].values.reshape(-1, 1)), sample_fake_news['type'])\n",
    "\n",
    "#Creating the advanced model choosing the best out of the three advanced models\n",
    "advanced_model = Pipeline([\n",
    "    ('Vectorizer', CountVectorizer()),\n",
    "    ('Transformer', TfidfTransformer(use_idf=False)),\n",
    "    ('Model', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                            alpha=1e-4, random_state=0,\n",
    "                            max_iter=5, tol=None)),\n",
    "    ])\n",
    "\n",
    "advanced_model.fit(sample_fake_news['content'], sample_fake_news['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIAR dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liar_data = pd.read_csv(\"C:\\\\Users\\\\Maher\\\\Documents\\\\GitHub\\\\Fake-News-Project\\\\test.tsv\", sep='\\t', header=None)[[1,2]]\n",
    "\n",
    "def binary_clean(text: str):\n",
    "    \"\"\"grouping data into fake and reliable\"\"\"\n",
    "    if text in [\"true\", \"mostly-true\"]: return True\n",
    "    elif text in [\"false\", \"pants-fire\"]: return False\n",
    "    else: return None\n",
    "\n",
    "#cleaning liar data\n",
    "liar_data[1] = liar_data[1].apply(binary_clean)\n",
    "liar_data[2] = liar_data[2].apply(tokenize_text).apply(stemming_no_stopwords)\n",
    "liar_data.dropna(subset=[1], inplace=True)\n",
    "liar_data.dropna(subset=[2], inplace=True)\n",
    "\n",
    "#creating liar test X and y data\n",
    "X_liar = liar_data[2]\n",
    "y_liar = liar_data[1].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test advanced model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating X and y for the complete data \n",
    "X_test = complete_fake_news['content'].tail(complete_len - sample_len)\n",
    "y_test = complete_fake_news[\"type\"].tail(complete_len - sample_len)\n",
    "\n",
    "simple_predictions_test = simple_model.predict(one_hot_encoder.fit_transform(X_test.values.reshape(-1, 1)))\n",
    "simple_predictions_liar = simple_model.predict(one_hot_encoder.fit_transform(X_liar.values.reshape(-1, 1))).tolist()\n",
    "\n",
    "advanced_predictions_test = advanced_model.predict(X_test)\n",
    "advanced_predictions_liar = advanced_model.predict(X_liar).tolist()\n",
    "\n",
    "print(tabulate([\n",
    "    [\"Simple\", \"X_liar\", f1_score(y_liar, simple_predictions_liar), accuracy_score(y_liar, simple_predictions_liar)],\n",
    "    [\"Simple\", \"X_liar\", f1_score(y_liar, simple_predictions_liar), accuracy_score(y_liar, simple_predictions_liar)],\n",
    "    [\"Advanced\", \"X_test\", f1_score(y_test, advanced_predictions_test), accuracy_score(y_test, advanced_predictions_test)],\n",
    "    [\"Advanced\", \"X_liar\", f1_score(y_liar, advanced_predictions_liar), accuracy_score(y_liar, advanced_predictions_liar)],\n",
    "    ], headers=[\"model\", \"test data\", \"f1\", \"Accuracy\"], tablefmt='orgtbl'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
