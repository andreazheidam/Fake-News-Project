{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1: Data Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1.1: Importing and cleaning the a sample of the FakeNewsCorpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import pandas as pd\n",
    "from tabulate import tabulate\n",
    "\n",
    "def clean_text(element):\n",
    "    \"\"\"cleans raw data using re.sub() to remove double newlines, space, and tabs. Also replace dates, emails, urls, and numbers\"\"\"\n",
    "    text = str(element).lower()\n",
    "    \n",
    "    num_pattern = re.compile(r\"(\\d+)\")\n",
    "    date_pattern = re.compile(r'\\b\\d{1,2}[/.-]\\d{1,2}[/.-]\\d{2,4}\\b') #using the dd-mm-yyyy format\n",
    "    email_pattern = re.compile(r'\\b[a-zA-Z0-9._-]+@[a-zA-Z0-9.-]+\\.[a-z|A-Z]{2,}\\b')\n",
    "    url_pattern = re.compile(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+')\n",
    "    \n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(date_pattern, \"<DATE>\", text)\n",
    "    text = re.sub(email_pattern, \"<EMAIL>\", text)\n",
    "    text = re.sub(url_pattern, \"<URL>\", text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(num_pattern, \"<NUM>\", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def unique_words(text: list, n: int):\n",
    "    \"\"\"returns a list of the n most used words in the list\"\"\"\n",
    "    words = {}\n",
    "    for word in text:\n",
    "        if word in words.keys():\n",
    "            words[word] += 1\n",
    "        else:\n",
    "            words[word] = 1\n",
    "    \n",
    "    #sorting the dictionary into a list of tuples by word count in descending order\n",
    "    return (sorted(words.items(), key = lambda x:x[1], reverse = True)[0:n])\n",
    "\n",
    "def remove_stopwords(unfiltered_text: list):\n",
    "    \"\"\"remove stopwords from list of strings\"\"\"\n",
    "    filtered_words = []\n",
    "    for word in unfiltered_text: \n",
    "        if word not in stopwords.words('english'): filtered_words.append(word)\n",
    "    return filtered_words\n",
    "\n",
    "def word_vocabulary_analasis(text: list):\n",
    "    \"\"\"print reduction rate of vocabulary of tokenzied data before and after removing stopwords and stemming\"\"\"\n",
    "    \n",
    "    unique_words = set(text)\n",
    "    stemmed_words = set([PorterStemmer().stem(word) for word in unique_words])\n",
    "    stop_words = remove_stopwords(unique_words)\n",
    "    stop_stemmed_words = remove_stopwords(stemmed_words)\n",
    "    \n",
    "    len_unique_words = len(unique_words)\n",
    "    len_stemmed_words = len(stemmed_words)\n",
    "    len_stop_words = len(stop_words)\n",
    "    len_stop_stemmed_words = len(stop_stemmed_words)\n",
    "    \n",
    "    #printing reduction rate table\n",
    "    print(tabulate([['stemming', 100 * (len_unique_words - len_stemmed_words) / len_unique_words], \n",
    "                    ['removing stopwords', 100 * (len_unique_words - len_stop_words) / len_unique_words], \n",
    "                    ['stemming and removing stopwords', 100 * (len_unique_words - len_stop_stemmed_words) / len_unique_words]], \n",
    "                   headers=['data cleaning type', 'reduction rate of vocabularyin percent'], tablefmt='orgtbl'))\n",
    "\n",
    "def stemming_data(text: str):\n",
    "    \"\"\"returns stemmed data where stopwords are removed from string\"\"\"\n",
    "    unique_words = set(text.split())\n",
    "    stemmed_words = set([PorterStemmer().stem(word) for word in unique_words])\n",
    "    return remove_stopwords(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "#importing the CVS file as a pandas Dataframe and saving a copy locally\n",
    "if os.path.exists(os.getcwd() + \"//news_data.csv\"): \n",
    "    raw_data_fake_news = pd.read_csv(os.getcwd() + \"//news_data.csv\")\n",
    "else:\n",
    "    raw_data_fake_news = pd.read_csv(\"https://raw.githubusercontent.com/several27/FakeNewsCorpus/master/news_sample.csv\")\n",
    "    with open(os.getcwd() + \"//news_data.csv\", \"w\") as file:\n",
    "        raw_data_fake_news.to_csv(os.getcwd() + \"//news_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning and basic analasis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| data cleaning type              |   reduction rate of vocabularyin percent |\n",
      "|---------------------------------+------------------------------------------|\n",
      "| stemming                        |                                32.8514   |\n",
      "| removing stopwords              |                                 0.795229 |\n",
      "| stemming and removing stopwords |                                33.502    |\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>domain</th>\n",
       "      <th>type</th>\n",
       "      <th>url</th>\n",
       "      <th>content</th>\n",
       "      <th>title</th>\n",
       "      <th>authors</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>awmcom</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>URL</td>\n",
       "      <td>sometimes the power of christmas will make you...</td>\n",
       "      <td>church congregation brings gift to waitresses ...</td>\n",
       "      <td>ruth harris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beforeitsnewscom</td>\n",
       "      <td>fake</td>\n",
       "      <td>URL</td>\n",
       "      <td>awakening of &lt;NUM&gt; strands of dna  reconnectin...</td>\n",
       "      <td>awakening of &lt;NUM&gt; strands of dna  reconnectin...</td>\n",
       "      <td>zurich times</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>cnnnextcom</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>URL</td>\n",
       "      <td>never hike alone a friday the &lt;NUM&gt;th fan film...</td>\n",
       "      <td>never hike alone  a friday the &lt;NUM&gt;th fan fil...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>awmcom</td>\n",
       "      <td>unreliable</td>\n",
       "      <td>URL</td>\n",
       "      <td>when a rare shark was caught scientists were l...</td>\n",
       "      <td>elusive alien of the sea  caught by scientist ...</td>\n",
       "      <td>alexander smith</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>bipartisanreportcom</td>\n",
       "      <td>clickbait</td>\n",
       "      <td>URL</td>\n",
       "      <td>donald trump has the unnerving ability to abil...</td>\n",
       "      <td>trumps genius poll is complete  the results ha...</td>\n",
       "      <td>gloria christie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>245</th>\n",
       "      <td>beforeitsnewscom</td>\n",
       "      <td>fake</td>\n",
       "      <td>URL</td>\n",
       "      <td>prison for rahm gods work and many others head...</td>\n",
       "      <td>prison for rahm gods work and many others</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>246</th>\n",
       "      <td>beforeitsnewscom</td>\n",
       "      <td>fake</td>\n",
       "      <td>URL</td>\n",
       "      <td>&lt;NUM&gt; useful items for your tiny home headline...</td>\n",
       "      <td>&lt;NUM&gt; useful items for your tiny home</td>\n",
       "      <td>dimitry k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>247</th>\n",
       "      <td>wwwnewsmaxcom</td>\n",
       "      <td>nan</td>\n",
       "      <td>URL</td>\n",
       "      <td>former cia director michael hayden said thursd...</td>\n",
       "      <td>michael hayden we should be frightened by trum...</td>\n",
       "      <td>todd beamon</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>248</th>\n",
       "      <td>wwwnewsmaxcom</td>\n",
       "      <td>nan</td>\n",
       "      <td>URL</td>\n",
       "      <td>antonio sabato jr says hollywoods liberal elit...</td>\n",
       "      <td>antonio sabato jr its oprah or bust for hollyw...</td>\n",
       "      <td>bill hoffmann</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>wwwnewsmaxcom</td>\n",
       "      <td>nan</td>\n",
       "      <td>URL</td>\n",
       "      <td>former us president bill clinton on monday cal...</td>\n",
       "      <td>bill clinton calls for release of reuters jour...</td>\n",
       "      <td>nan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>250 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  domain        type  url  \\\n",
       "0                 awmcom  unreliable  URL   \n",
       "1       beforeitsnewscom        fake  URL   \n",
       "2             cnnnextcom  unreliable  URL   \n",
       "3                 awmcom  unreliable  URL   \n",
       "4    bipartisanreportcom   clickbait  URL   \n",
       "..                   ...         ...  ...   \n",
       "245     beforeitsnewscom        fake  URL   \n",
       "246     beforeitsnewscom        fake  URL   \n",
       "247        wwwnewsmaxcom         nan  URL   \n",
       "248        wwwnewsmaxcom         nan  URL   \n",
       "249        wwwnewsmaxcom         nan  URL   \n",
       "\n",
       "                                               content  \\\n",
       "0    sometimes the power of christmas will make you...   \n",
       "1    awakening of <NUM> strands of dna  reconnectin...   \n",
       "2    never hike alone a friday the <NUM>th fan film...   \n",
       "3    when a rare shark was caught scientists were l...   \n",
       "4    donald trump has the unnerving ability to abil...   \n",
       "..                                                 ...   \n",
       "245  prison for rahm gods work and many others head...   \n",
       "246  <NUM> useful items for your tiny home headline...   \n",
       "247  former cia director michael hayden said thursd...   \n",
       "248  antonio sabato jr says hollywoods liberal elit...   \n",
       "249  former us president bill clinton on monday cal...   \n",
       "\n",
       "                                                 title          authors  \n",
       "0    church congregation brings gift to waitresses ...      ruth harris  \n",
       "1    awakening of <NUM> strands of dna  reconnectin...     zurich times  \n",
       "2    never hike alone  a friday the <NUM>th fan fil...              nan  \n",
       "3    elusive alien of the sea  caught by scientist ...  alexander smith  \n",
       "4    trumps genius poll is complete  the results ha...  gloria christie  \n",
       "..                                                 ...              ...  \n",
       "245          prison for rahm gods work and many others              nan  \n",
       "246              <NUM> useful items for your tiny home        dimitry k  \n",
       "247  michael hayden we should be frightened by trum...      todd beamon  \n",
       "248  antonio sabato jr its oprah or bust for hollyw...    bill hoffmann  \n",
       "249  bill clinton calls for release of reuters jour...              nan  \n",
       "\n",
       "[250 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#keeping only the relevant collumns\n",
    "raw_data_fake_news = raw_data_fake_news[[\"domain\", \"type\", \"url\", \"content\", \"title\", \"authors\"]]\n",
    "\n",
    "#cleaning raw text using clean_text function on all elements\n",
    "tokenized_data_fake_news = raw_data_fake_news.map(clean_text)\n",
    "\n",
    "#doing a word analasis and getting the stemmed data without stopwords\n",
    "list_of_words_content = [\"content\"]\n",
    "for text in tokenized_data_fake_news[\"content\"].tolist():\n",
    "    for word in text.split():\n",
    "        list_of_words_content.append(word)\n",
    "\n",
    "word_vocabulary_analasis(list_of_words_content)\n",
    "\n",
    "#stemming and removing stopwords from the data\n",
    "stemmed_data_fake_news = tokenized_data_fake_news\n",
    "\n",
    "#printing the Dataframe\n",
    "tokenized_data_fake_news"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1.4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Simple Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Advanced Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 4: Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Conclusions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
