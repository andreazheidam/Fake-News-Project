{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"995K_subset.csv\", dtype={0: str, 1: str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique domains: 683\n"
     ]
    }
   ],
   "source": [
    "num_unique_domains = df['domain'].nunique()\n",
    "print(\"Number of unique domains:\", num_unique_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_category = {\n",
    "    \"fake\": \"fake\",\n",
    "    \"satire\": \"fake\",\n",
    "    \"bias\": \"fake\",\n",
    "    \"conspiracy\": \"fake\",\n",
    "    \"junksci\": \"fake\",\n",
    "    \"reliable\": \"reliable\",\n",
    "    \"political\": \"reliable\",\n",
    "    \"clickbait\": \"reliable\",\n",
    "}\n",
    "\n",
    "df['category'] = df['type'].apply(lambda x: label_to_category.get(x, 'omitted'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain\n",
      "100percentfedup.com                                        [bias]\n",
      "2016-11-13T15:38:41.407+02:00        [2018-02-10 13:43:39.521661]\n",
      "21stcenturywire.com                                  [conspiracy]\n",
      "21wire.tv                                                   [nan]\n",
      "4threvolutionarywar.wordpress.com                          [bias]\n",
      "                                                 ...             \n",
      "yournationnews.com                                    [clickbait]\n",
      "yournewswire.com                                      [clickbait]\n",
      "zeenews.india.com                                      [reliable]\n",
      "zerohedge.com                                        [conspiracy]\n",
      "zootfeed.com                                         [conspiracy]\n",
      "Name: type, Length: 683, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Group the DataFrame by 'domain' and aggregate the associated types\n",
    "domain_assigned_types_all = df.groupby('domain')['type'].unique()\n",
    "\n",
    "# Display the result\n",
    "print(domain_assigned_types_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the total count of articles for each domain\n",
    "domain_counts = df['domain'].value_counts().nlargest(10)\n",
    "\n",
    "# Filter the DataFrame to include only the top 10 domains\n",
    "top_10_domains_df = df[df['domain'].isin(domain_counts.index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain\n",
      "abovetopsecret.com    [conspiracy]\n",
      "beforeitsnews.com           [fake]\n",
      "dailykos.com           [political]\n",
      "express.co.uk              [rumor]\n",
      "nationalreview.com     [political]\n",
      "nytimes.com             [reliable]\n",
      "sputniknews.com             [bias]\n",
      "wikileaks.org         [unreliable]\n",
      "www.ammoland.com             [nan]\n",
      "www.newsmax.com              [nan]\n",
      "Name: type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Group the DataFrame by 'domain' and aggregate the associated types\n",
    "domain_assigned_types = top_10_domains_df.groupby('domain')['type'].unique()\n",
    "\n",
    "# Display the result\n",
    "print(domain_assigned_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "domain\n",
      "abovetopsecret.com    [conspiracy]\n",
      "beforeitsnews.com           [fake]\n",
      "dailykos.com           [political]\n",
      "express.co.uk              [rumor]\n",
      "nationalreview.com     [political]\n",
      "nytimes.com             [reliable]\n",
      "sputniknews.com             [bias]\n",
      "wikileaks.org         [unreliable]\n",
      "www.ammoland.com             [nan]\n",
      "www.newsmax.com              [nan]\n",
      "Name: type, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Group the DataFrame by 'domain' and aggregate the associated types\n",
    "domain_assigned_types_all = df.groupby('domain')['type'].unique()\n",
    "\n",
    "# Display the result\n",
    "print(domain_assigned_types)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category             fake  omitted  reliable\n",
      "domain                                      \n",
      "abovetopsecret.com  100.0      0.0       0.0\n",
      "beforeitsnews.com   100.0      0.0       0.0\n",
      "dailykos.com          0.0      0.0     100.0\n",
      "express.co.uk         0.0    100.0       0.0\n",
      "nationalreview.com    0.0      0.0     100.0\n",
      "nytimes.com           0.0      0.0     100.0\n",
      "sputniknews.com     100.0      0.0       0.0\n",
      "wikileaks.org         0.0    100.0       0.0\n",
      "www.ammoland.com      0.0    100.0       0.0\n",
      "www.newsmax.com       0.0    100.0       0.0\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total count of articles for each domain\n",
    "domain_counts = df['domain'].value_counts().nlargest(10)\n",
    "\n",
    "# Filter the DataFrame to include only the top 10 domains\n",
    "top_10_domains_df = df[df['domain'].isin(domain_counts.index)]\n",
    "\n",
    "# Group the filtered DataFrame by 'domain' and the categorization column and count the occurrences\n",
    "domain_type_counts = top_10_domains_df.groupby(['domain', 'category']).size().unstack(fill_value=0)\n",
    "\n",
    "# Calculate the total count of articles for each domain\n",
    "domain_counts = domain_type_counts.sum(axis=1)\n",
    "\n",
    "# Calculate the percentage distribution for each domain\n",
    "percentage_distribution = domain_type_counts.div(domain_counts, axis=0) * 100\n",
    "\n",
    "print(percentage_distribution)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
